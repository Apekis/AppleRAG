{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "17b64aa2",
      "metadata": {},
      "source": [
        "# ApplePay RAG Chatbot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38c65c75",
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "\n",
        "\n",
        "import re, time, json, pathlib, requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "APPLEPAY_ROOTS = [\n",
        "    \"https://www.apple.com/apple-pay/\",\n",
        "    \"https://support.apple.com/apple-pay\",\n",
        "    \"https://flutterwave.com/ke/support/payment-methods/apple-pay-frequently-asked-questions-faqs\",\n",
        "    \"https://aibgb.co.uk/apple-pay/apple-pay-faqs\",\n",
        "    \"https://www.americanexpress.com/us/credit-cards/features-benefits/digital-wallets/apple-pay/frequently-asked-questions.html\",\n",
        "    \"https://www.wellsfargo.com/help/mobile-features/apple-pay-faqs/\"\n",
        "    #\"https://horizonbank.com.au/help/faqs/apple-pay-faqs/\" blocked\n",
        "]\n",
        "\n",
        "def get_links(base_url):\n",
        "    resp = requests.get(base_url, timeout=20)\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "    links = set()\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"]\n",
        "        if href.startswith(\"#\"): continue\n",
        "        full = urljoin(base_url, href)\n",
        "        if \"apple.com\" in full and (\"apple-pay\" in full or \"support.apple.com\" in full):\n",
        "            links.add(full)\n",
        "    return sorted(links)\n",
        "\n",
        "def fetch_page(url):\n",
        "    r = requests.get(url, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "\n",
        "def clean_text(html):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    for tag in soup([\"script\",\"style\",\"noscript\"]):\n",
        "        tag.decompose()\n",
        "    text = soup.get_text(\"\\n\")\n",
        "    text = re.sub(r\"\\n{2,}\", \"\\n\", text).strip()\n",
        "    title = soup.title.string.strip() if soup.title else \"\"\n",
        "    return {\"title\": title, \"text\": text}\n",
        "\n",
        "output_dir=\"data/raw\"\n",
        "pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "visited = set()\n",
        "pages = []\n",
        "for root in APPLEPAY_ROOTS:\n",
        "    links = get_links(root)\n",
        "    for url in links:\n",
        "        if url in visited: continue\n",
        "        visited.add(url)\n",
        "        try:\n",
        "            html = fetch_page(url)\n",
        "            cleaned = clean_text(html)\n",
        "            pages.append({\"url\": url, **cleaned})\n",
        "            time.sleep(0.5)\n",
        "        except Exception as e:\n",
        "            print(\"Failed:\", url, e)\n",
        "outpath = pathlib.Path(output_dir) / \"bs4_pages.json\"\n",
        "with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(pages, f, ensure_ascii=False, indent=2)\n",
        "print(f\"BS4 scraped {len(pages)} pages → {outpath}\")\n",
        "\n",
        "#pages/sec 74/1m\n",
        "#failures 1/7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "741a9c33",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fail: https://www.wellsfargo.com/help/mobile-features/apple-pay-faqs/https://horizonbank.com.au/help/faqs/apple-pay-faqs/\n",
            "Trafilatura scraped 5 pages → data/raw/trafilatura_pages.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import trafilatura\n",
        "\n",
        "URLS = [\n",
        "    \"https://www.apple.com/apple-pay/\",\n",
        "    \"https://support.apple.com/apple-pay\",\n",
        "    \"https://flutterwave.com/ke/support/payment-methods/apple-pay-frequently-asked-questions-faqs\",\n",
        "    \"https://aibgb.co.uk/apple-pay/apple-pay-faqs\",\n",
        "    \"https://www.americanexpress.com/us/credit-cards/features-benefits/digital-wallets/apple-pay/frequently-asked-questions.html\",\n",
        "    \"https://www.wellsfargo.com/help/mobile-features/apple-pay-faqs/\"\n",
        "    \"https://horizonbank.com.au/help/faqs/apple-pay-faqs/\"\n",
        "]\n",
        "\n",
        "output_dir=\"data/raw\"\n",
        "pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "pages = []\n",
        "for url in URLS:\n",
        "    downloaded = trafilatura.fetch_url(url)\n",
        "    if not downloaded:\n",
        "        print(\"Fail:\", url); continue\n",
        "    text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)\n",
        "    pages.append({\"url\": url, \"title\": \"\", \"text\": text or \"\"})\n",
        "outpath = pathlib.Path(output_dir) / \"trafilatura_pages.json\"\n",
        "with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(pages, f, ensure_ascii=False, indent=2)\n",
        "print(f\"Trafilatura scraped {len(pages)} pages → {outpath}\")\n",
        "\n",
        "#pages/sec: 5/4s\n",
        "#failure:1/7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9e15924",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Scrape Quality Report ===\n",
            "\n",
            "Method: BeautifulSoup\n",
            "Pages: 74\n",
            "Tokens: 295217\n",
            "Avg Noise Ratio: 0.0695\n",
            "\n",
            "Method: Trafilatura\n",
            "Pages: 5\n",
            "Tokens: 4493\n",
            "Avg Noise Ratio: 0.0281\n",
            "\n",
            "Report saved to data/raw/scrape_quality_report.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def load_pages(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def count_tokens(text):\n",
        "    # Simple token count by whitespace split\n",
        "    return len(text.split())\n",
        "\n",
        "def noise_ratio(text):\n",
        "    # Heuristic: ratio of non-alphanumeric characters to total characters\n",
        "    if not text:\n",
        "        return 1.0\n",
        "    non_alpha = len(re.findall(r\"[^a-zA-Z0-9\\s]\", text))\n",
        "    return round(non_alpha / len(text), 4)\n",
        "\n",
        "def report(method_name, pages):\n",
        "    num_pages = len(pages)\n",
        "    total_tokens = 0\n",
        "    noise_scores = []\n",
        "    for p in pages:\n",
        "        txt = p.get(\"text\", \"\")\n",
        "        total_tokens += count_tokens(txt)\n",
        "        noise_scores.append(noise_ratio(txt))\n",
        "    avg_noise = round(sum(noise_scores) / len(noise_scores), 4) if noise_scores else 0\n",
        "    return {\n",
        "        \"method\": method_name,\n",
        "        \"#pages\": num_pages,\n",
        "        \"#tokens\": total_tokens,\n",
        "        \"avg_noise_ratio\": avg_noise\n",
        "    }\n",
        "\n",
        "base_dir = pathlib.Path(\"data/raw\")\n",
        "bs4_file = base_dir / \"bs4_pages.json\"\n",
        "tra_file = base_dir / \"trafilatura_pages.json\"\n",
        "\n",
        "bs4_pages = load_pages(bs4_file)\n",
        "tra_pages = load_pages(tra_file)\n",
        "\n",
        "bs4_report = report(\"BeautifulSoup\", bs4_pages)\n",
        "tra_report = report(\"Trafilatura\", tra_pages)\n",
        "\n",
        "print(\"\\n=== Scrape Quality Report ===\")\n",
        "for r in [bs4_report, tra_report]:\n",
        "    print(f\"\\nMethod: {r['method']}\")\n",
        "    print(f\"Pages: {r['#pages']}\")\n",
        "    print(f\"Tokens: {r['#tokens']}\")\n",
        "    print(f\"Avg Noise Ratio: {r['avg_noise_ratio']}\")\n",
        "\n",
        "# Optional: Save to JSON\n",
        "out_path = base_dir / \"scrape_quality_report.json\"\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"BeautifulSoup\": bs4_report, \"Trafilatura\": tra_report}, f, indent=2)\n",
        "print(f\"\\nReport saved to {out_path}\")\n",
        "\n",
        "\n",
        "# === Scrape Quality Report ===\n",
        "\n",
        "# Method: BeautifulSoup\n",
        "# Pages: 74\n",
        "# Tokens: 295217\n",
        "# Avg Noise Ratio: 0.0695\n",
        "\n",
        "# Method: Trafilatura\n",
        "# Pages: 5\n",
        "# Tokens: 4493\n",
        "# Avg Noise Ratio: 0.0281\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23bd46eb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized 77 pages → data/processed/pages.json\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "\n",
        "in_files=[\"data/raw/bs4_pages.json\"]\n",
        "out_file=\"data/processed/pages.json\"\n",
        "pathlib.Path(out_file).parent.mkdir(parents=True, exist_ok=True)\n",
        "rows = []\n",
        "for f in in_files:\n",
        "    with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
        "        data = json.load(fh)\n",
        "        for d in data:\n",
        "            text = d.get(\"text\")\n",
        "            if not text and d.get(\"html\"):\n",
        "                # crude HTML→text fallback\n",
        "                from bs4 import BeautifulSoup\n",
        "                soup = BeautifulSoup(d[\"html\"], \"html.parser\")\n",
        "                text = soup.get_text(\"\\n\")\n",
        "            if not text: continue\n",
        "            rows.append({\n",
        "                \"url\": d[\"url\"],\n",
        "                \"title\": d.get(\"title\",\"\"),\n",
        "                \"text\": text.strip()\n",
        "            })\n",
        "with open(out_file, \"w\", encoding=\"utf-8\") as out:\n",
        "    json.dump(rows, out, ensure_ascii=False, indent=2)\n",
        "print(f\"Normalized {len(rows)} pages → {out_file}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b8f154a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "/tmp/ipykernel_545096/882148970.py:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  heading_tags = ('\\h1','\\h2','\\h3','\\h4','\\h5','\\h6')\n",
            "/tmp/ipykernel_545096/882148970.py:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  heading_tags = ('\\h1','\\h2','\\h3','\\h4','\\h5','\\h6')\n",
            "/tmp/ipykernel_545096/882148970.py:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  heading_tags = ('\\h1','\\h2','\\h3','\\h4','\\h5','\\h6')\n",
            "/tmp/ipykernel_545096/882148970.py:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  heading_tags = ('\\h1','\\h2','\\h3','\\h4','\\h5','\\h6')\n",
            "/tmp/ipykernel_545096/882148970.py:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  heading_tags = ('\\h1','\\h2','\\h3','\\h4','\\h5','\\h6')\n",
            "/tmp/ipykernel_545096/882148970.py:102: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  heading_tags = ('\\h1','\\h2','\\h3','\\h4','\\h5','\\h6')\n",
            "/home/andreas_koutsopoulos/LLM/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/andreas_koutsopoulos/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#chunkers\n",
        "#fixed size chunker----------------------------------------------\n",
        "def fixed_chunk_text(text:str, chunk_size:int=512, overlap:int=0):\n",
        "    words = text.split()\n",
        "    chunks=[]; i=0\n",
        "    while i < len(words):\n",
        "        chunks.append(\" \".join(words[i:i+chunk_size]))\n",
        "        i += max(1, chunk_size-overlap)\n",
        "    return chunks\n",
        "\n",
        "#semantic chunker------------------------------------------------------------------\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def semantic_chunk_text(text, similarity_threshold=0.8, max_tokens=500):\n",
        "    \"\"\"\n",
        "    Splits text into semantic chunks based on sentence similarity and max token length.\n",
        "    \"\"\"\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    if not sentences:\n",
        "        return []\n",
        "    embeddings = semantic_model.encode(sentences)\n",
        "    chunks = []\n",
        "    current_chunk = [sentences[0]]\n",
        "    current_embedding = embeddings[0]\n",
        "    for i in range(1, len(sentences)):\n",
        "        sim = cosine_similarity([current_embedding], [embeddings[i]])[0][0]\n",
        "        chunk_token_count = len(\" \".join(current_chunk)) // 4\n",
        "        if sim >= similarity_threshold and chunk_token_count < max_tokens:\n",
        "            current_chunk.append(sentences[i])\n",
        "            current_embedding = (current_embedding + embeddings[i]) / 2\n",
        "        else:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [sentences[i]]\n",
        "            current_embedding = embeddings[i]\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "#recursive chunker------------------------------------------------------------------\n",
        "\n",
        "def recursive_chunk_text(text, max_chunk_size=1000):\n",
        "    \"\"\"\n",
        "    Recursively splits a block of text into chunks that fit within size constraints.\n",
        "    Tries splitting by sections, then newlines, then sentences.\n",
        "    \"\"\"\n",
        "    import nltk\n",
        "    nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "    def split_chunk(chunk):\n",
        "        if len(chunk) <= max_chunk_size:\n",
        "            return [chunk]\n",
        "        # Try splitting by double newlines\n",
        "        sections = chunk.split(\"\\n\\n\")\n",
        "        if len(sections) > 1:\n",
        "            result = []\n",
        "            for section in sections:\n",
        "                if section.strip():\n",
        "                    result.extend(split_chunk(section.strip()))\n",
        "            return result\n",
        "        # Try splitting by single newline\n",
        "        sections = chunk.split(\"\\n\")\n",
        "        if len(sections) > 1:\n",
        "            result = []\n",
        "            for section in sections:\n",
        "                if section.strip():\n",
        "                    result.extend(split_chunk(section.strip()))\n",
        "            return result\n",
        "        # Fallback: split by sentences\n",
        "        sentences = nltk.sent_tokenize(chunk)\n",
        "        chunks, current_chunk, current_size = [], [], 0\n",
        "        for sentence in sentences:\n",
        "            if current_size + len(sentence) > max_chunk_size:\n",
        "                if current_chunk:\n",
        "                    chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_size = len(sentence)\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_size += len(sentence)\n",
        "        if current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "        return chunks\n",
        "\n",
        "    return split_chunk(text)\n",
        "\n",
        "#structure based chunker------------------------------------------------------------------\n",
        "\n",
        "def structure_chunk_text(text):\n",
        "    \"\"\"\n",
        "    Splits text into chunks based on detected headings (e.g., CHAPTER, section numbers).\n",
        "    \"\"\"\n",
        "    lines = text.split(\"\\n\")\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    heading_tags = ('\\h1','\\h2','\\h3','\\h4','\\h5','\\h6')\n",
        "    for line in lines:\n",
        "        if any(heading in (line.strip()) for heading in heading_tags) and current_chunk:\n",
        "            chunks.append(\"\\n\".join(current_chunk))\n",
        "            current_chunk = [line]\n",
        "        else:\n",
        "            current_chunk.append(line)\n",
        "    if current_chunk:\n",
        "        chunks.append(\"\\n\".join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "#LLM based chunker------------------------------------------------------------------\n",
        "from openai import OpenAI\n",
        "import os\n",
        "def llm_chunk_text(text, chunk_size=1000, model=\"gpt-4o-mini\", api_key=None):\n",
        "    \"\"\"\n",
        "    Uses an LLM to find semantically coherent chunk boundaries around a target chunk size.\n",
        "    \"\"\"\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    def get_chunk_boundary(text_segment):\n",
        "        prompt = f\"\"\"\n",
        "        Analyze the following text and identify the best point to split it\n",
        "        into two semantically coherent parts. The split should occur near {chunk_size} characters.\n",
        "        Text:\n",
        "        \\\"\\\"\\\"{text_segment}\\\"\\\"\\\"\n",
        "        Return only the integer index (character position) within this text where the split should occur.\n",
        "        \"\"\"\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a text analysis expert.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0\n",
        "        )\n",
        "        split_str = response.choices[0].message.content.strip()\n",
        "        try:\n",
        "            split_point = int(split_str)\n",
        "        except ValueError:\n",
        "            split_point = chunk_size\n",
        "        return split_point\n",
        "\n",
        "    chunks = []\n",
        "    remaining_text = text\n",
        "    while len(remaining_text) > chunk_size:\n",
        "        text_window = remaining_text[:chunk_size * 2]\n",
        "        split_point = get_chunk_boundary(text_window)\n",
        "        if split_point < 100 or split_point > len(text_window) - 100:\n",
        "            split_point = chunk_size\n",
        "        chunks.append(remaining_text[:split_point].strip())\n",
        "        remaining_text = remaining_text[split_point:].strip()\n",
        "    if remaining_text:\n",
        "        chunks.append(remaining_text)\n",
        "    return chunks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "65238d2d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Fixed chunking complete: 802 chunks saved to data/chunks/fixed_chunks.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Semantic chunking complete: 6318 chunks saved to data/chunks/semantic_chunks.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Paths\n",
        "INPUT_FILE = \"data/processed/pages.json\"\n",
        "OUTPUT_DIR = Path(\"data/chunks\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def load_pages(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def chunk_page(page, strategy=\"fixed\", **kwargs):\n",
        "    text = page.get(\"text\", \"\")\n",
        "    if not text.strip():\n",
        "        return []\n",
        "    if strategy == \"fixed\":\n",
        "        chunks = fixed_chunk_text(text, chunk_size=kwargs.get(\"chunk_size\", 512),overlap=kwargs.get(\"overlap\", 64))\n",
        "    elif strategy == \"semantic\":\n",
        "        chunks = semantic_chunk_text(text,\n",
        "                                     similarity_threshold=kwargs.get(\"similarity_threshold\", 0.8),\n",
        "                                     max_tokens=kwargs.get(\"max_tokens\", 500))\n",
        "    elif strategy == \"recursive\":\n",
        "        chunks = recursive_chunk_text(text,\n",
        "                                      max_chunk_size=kwargs.get(\"max_chunk_size\", 1000))\n",
        "    elif strategy == \"structure\":\n",
        "        chunks = structure_chunk_text(text)\n",
        "    elif strategy == \"llm\":\n",
        "        chunks = llm_chunk_text(text,\n",
        "                                chunk_size=kwargs.get(\"chunk_size\", 1000),\n",
        "                                model=kwargs.get(\"model\", \"gpt-4o-mini\"),\n",
        "                                api_key=kwargs.get(\"api_key\"))\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "    # Wrap chunks with metadata\n",
        "    return [\n",
        "        {\n",
        "            \"url\": page.get(\"url\"),\n",
        "            \"title\": page.get(\"title\"),\n",
        "            \"chunk_index\": i,\n",
        "            \"chunk\": c,\n",
        "            \"chunk_char_count\": len(c),\n",
        "            \"chunk_word_count\": len(c.split()),\n",
        "            \"chunk_token_count\": round(len(c) / 4, 2),  # approx tokens\n",
        "            \"strategy\": strategy\n",
        "        }\n",
        "        for i, c in enumerate(chunks)\n",
        "    ]\n",
        "\n",
        "def process_pages(strategy=\"fixed\", **kwargs):\n",
        "    pages = load_pages(INPUT_FILE)\n",
        "    all_chunks = []\n",
        "    for page in pages:\n",
        "        all_chunks.extend(chunk_page(page, strategy=strategy, **kwargs))\n",
        "    # Save chunks\n",
        "    out_file = OUTPUT_DIR / f\"{strategy}_chunks.json\"\n",
        "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"✅ {strategy.capitalize()} chunking complete: {len(all_chunks)} chunks saved to {out_file}\")\n",
        "\n",
        "# Example runs:\n",
        "process_pages(strategy=\"fixed\", chunk_size=512, overlap=128)\n",
        "process_pages(strategy=\"semantic\", similarity_threshold=0.75, max_tokens=500)\n",
        "#process_pages(strategy=\"recursive\", max_chunk_size=800)\n",
        "#process_pages(strategy=\"structure\")\n",
        "#process_pages(strategy=\"llm\", chunk_size=800, model=\"gpt-4o-mini\", api_key=\"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\")\n",
        "\n",
        "#chunk sizes\n",
        "#Fixed chunking complete: 9725 chunks saved to data\\chunks\\fixed_chunks.json  2s\n",
        "#Llm chunking complete: 5996 chunks saved to data\\chunks\\llm_chunks.json 43m\n",
        "#semantic 6539  21m\n",
        "#recursive: 55213 \n",
        "#structure: 365 \n",
        "#fixed size: 256,0: 1209, 256,64: 1598, 256,128: 2380,\n",
        "#  512,0: 631, 512,64: 712, 512,128: 816,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "198d40da",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ FAISS index built with 77 chunks → data/index/faiss\n"
          ]
        }
      ],
      "source": [
        "#emdedings\n",
        "from backend.embeddings.e5_embed import embed_texts as e5_embed\n",
        "from backend.embeddings.bge_embed import embed_texts as bge_embed\n",
        "from backend.vectorstore.chroma_store import get_chroma, upsert_docs\n",
        "from backend.embeddings.minilm_embed import embed_texts as minilm_embed\n",
        "from backend.vectorstore.faiss_store import FaissStore\n",
        "import faiss\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# load chunks\n",
        "chunk_file=\"data/chunks/llm_chunks.json\"\n",
        "with open(chunk_file,\"r\",encoding=\"utf-8\") as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "#embed\n",
        "\n",
        "embed_fn = e5_embed\n",
        "vectors_e5 = embed_fn([c[\"chunk\"] for c in chunks])\n",
        "# Save embeddings to file\n",
        "embeddings_df = pd.DataFrame(vectors_e5)\n",
        "embeddings_df_save_path = \"vectors_e5.csv\"\n",
        "embeddings_df.to_csv(embeddings_df_save_path, index=False)\n",
        "vectors=vectors_e5\n",
        "indexfile=\"index_e5.faiss\"\n",
        "docsfile=\"docs_e5.json\"\n",
        "\n",
        "# embed_fn = bge_embed\n",
        "# vectors_bge = embed_fn([c[\"chunk\"] for c in chunks])\n",
        "# # Save embeddings to file\n",
        "# embeddings_df = pd.DataFrame(vectors_bge)\n",
        "# embeddings_df_save_path = \"vectors_bge.csv\"\n",
        "# embeddings_df.to_csv(embeddings_df_save_path, index=False)\n",
        "# vectors=vectors_bge\n",
        "# indexfile=\"index_bge.faiss\"\n",
        "# docsfile=\"docs_e5.json\"\n",
        "\n",
        "# embed_fn = minilm_embed\n",
        "# vectors_minilm = embed_fn([c[\"chunk\"] for c in chunks])\n",
        "# # Save embeddings to file\n",
        "# embeddings_df = pd.DataFrame(vectors_minilm)\n",
        "# embeddings_df_save_path = \"vectors_minilm.csv\"\n",
        "# embeddings_df.to_csv(embeddings_df_save_path, index=False)\n",
        "# vectors=vectors_minilm\n",
        "# indexfile=\"index_minilm.faiss\"\n",
        "# docsfile=\"docs_minilm.json\"\n",
        "\n",
        "#store\n",
        "\n",
        "index_dir = \"data/index/faiss\"\n",
        "os.makedirs(index_dir, exist_ok=True)\n",
        "\n",
        "dim = len(vectors[0])\n",
        "index = faiss.IndexFlatIP(dim)  # cosine similarity with normalized vectors\n",
        "emb = np.array(vectors).astype(\"float32\")\n",
        "faiss.normalize_L2(emb)\n",
        "index.add(emb)\n",
        "\n",
        "# Save index\n",
        "faiss.write_index(index, os.path.join(index_dir, indexfile))\n",
        "\n",
        "# Save docs metadata\n",
        "docs_path = os.path.join(index_dir, docsfile)\n",
        "with open(docs_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✅ FAISS index built with {len(chunks)} chunks → {index_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ea95e2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from typing import Dict, List, Tuple\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "def retrieve(query: str, top_k: int = 6):\n",
        "    #embed_fn = e5_embed\n",
        "    qv = embed_fn([query])[0]\n",
        "    results = []\n",
        "\n",
        "    # FAISS retrieval\n",
        "    index_dir = \"data/index/faiss\"\n",
        "    index_path = os.path.join(index_dir, indexfile)\n",
        "    docs_path = os.path.join(index_dir, docsfile)\n",
        "\n",
        "    if not os.path.exists(index_path) or not os.path.exists(docs_path):\n",
        "        raise FileNotFoundError(\"FAISS index or docs.json not found. Run build_index first.\")\n",
        "\n",
        "    # Load FAISS index\n",
        "    import faiss\n",
        "    import numpy as np\n",
        "    with open(docs_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        docs = json.load(f)\n",
        "\n",
        "    index = faiss.read_index(index_path)\n",
        "    q = np.array([qv]).astype(\"float32\")\n",
        "    faiss.normalize_L2(q)\n",
        "    D, I = index.search(q, top_k)\n",
        "\n",
        "    for rank, idx in enumerate(I[0]):\n",
        "        doc = docs[idx]\n",
        "        results.append({\n",
        "            \"text\": doc.get(\"chunk\", doc.get(\"chunk_text\", \"\")),\n",
        "            \"meta\": {\n",
        "                \"url\": doc.get(\"url\"),\n",
        "                \"title\": doc.get(\"title\"),\n",
        "                \"strategy\": doc.get(\"strategy\", \"unknown\"),\n",
        "                \"score\": float(D[0][rank])\n",
        "            }\n",
        "        })\n",
        "    return results\n",
        "\n",
        "\n",
        "def answer(query: str, contexts: List[Dict]) -> Dict:\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    sys = \"You are a customer support assistant for Apple Pay. Answer strictly using the provided context and cite sources.\"\n",
        "    citations_md = \"\\n\\n\".join([f\"[{i+1}] {c['meta']['title'] or c['meta']['url']} — {c['meta']['url']}\" for i, c in enumerate(contexts)])\n",
        "    context_text = \"\\n\\n---\\n\\n\".join([c[\"text\"] for c in contexts])\n",
        "\n",
        "    prompt = f\"\"\"Question: {query}\n",
        "\n",
        "Context (use only this information):\n",
        "{context_text}\n",
        "\n",
        "Citations:\n",
        "{citations_md}\n",
        "\n",
        "Instructions:\n",
        "- If an answer is not present in the context, say you don't know and suggest contacting Apple support.\n",
        "- Include bracketed citation numbers like [1], [2] inline where relevant.\n",
        "- Keep answers concise and accurate.\"\"\"\n",
        "\n",
        "    t0 = time.time()\n",
        "    resp = client.chat.completions.create(\n",
        "        model=os.getenv(\"OPENAI_CHAT_MODEL\",\"gpt-4o-mini\"),\n",
        "        messages=[{\"role\":\"system\",\"content\":sys},\n",
        "                  {\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0.0\n",
        "    )\n",
        "    dt = int((time.time() - t0) * 1000)\n",
        "    content = resp.choices[0].message.content\n",
        "    usage = resp.usage\n",
        "    return {\n",
        "        \"answer\": content,\n",
        "        \"citations\": [{\"index\": i+1, \"url\": c[\"meta\"][\"url\"], \"title\": c[\"meta\"][\"title\"], \"snippet\": c[\"text\"][:240]} for i, c in enumerate(contexts)],\n",
        "        \"latency_ms\": dt,\n",
        "        \"usage\": {\"prompt_tokens\": usage.prompt_tokens, \"completion_tokens\": usage.completion_tokens, \"total_tokens\": usage.total_tokens}\n",
        "    }\n",
        "\n",
        "#e5_embed: 18.9MB, Latency P50: 2315.5 ms; P95: 6044 ms\n",
        "#bge: 9.4MB\n",
        "#minilim: 9.4MB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "481c32ff",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: How do I add a new card to Apple Pay?\n",
            "A: To add a new card to Apple Pay, follow these steps based on your device:\n",
            "\n",
            "- **Mac**: On models with Touch ID, go to System Preferences, then Wallet & Apple Pay, and tap Add Card.\n",
            "  \n",
            "- **iPhone**: Open the Wallet app and ...\n",
            "Latency: 6044 ms\n",
            "---\n",
            "Q: What fees apply when using Apple Pay?\n",
            "A: Apple does not charge any fees when you pay with Apple Pay, whether in stores, online, or in apps [1][4]....\n",
            "Latency: 1074 ms\n",
            "---\n",
            "Q: How can I request a refund for an Apple Pay purchase?\n",
            "A: To request a refund for an Apple Pay purchase, you can handle it in the same way as you would for other card purchases made using Apple Pay. This includes making changes to your order, returns, or refunds as per the merc...\n",
            "Latency: 2193 ms\n",
            "---\n",
            "Q: Is Apple Pay supported on Apple Watch without iPhone?\n",
            "A: The provided context does not specify whether Apple Pay can be used on the Apple Watch without an iPhone. It mentions that you can complete purchases using Apple Pay on a compatible iPhone or Apple Watch, but it implies ...\n",
            "Latency: 2724 ms\n",
            "---\n",
            "Q: What are Apple Pay security features like tokenization?\n",
            "A: Apple Pay incorporates several security features, including tokenization, to protect your transactions. It uses built-in security features from the hardware and software of your device, ensuring a secure payment process....\n",
            "Latency: 3515 ms\n",
            "---\n",
            "Q: How do merchants integrate Apple Pay on the web?\n",
            "A: Merchants can integrate Apple Pay on the web by working with e-commerce platforms that support simple, web-based integration. This allows them to offer Apple Pay as a payment option to customers using compatible browsers...\n",
            "Latency: 2438 ms\n",
            "---\n",
            "Q: Where can I find KYC or identity verification requirements?\n",
            "A: You can find KYC or identity verification requirements by verifying your information with your bank or card issuer. They may ask you to provide details such as your name and address to their identity verification service...\n",
            "Latency: 1390 ms\n",
            "---\n",
            "Q: How do I remove a card from Apple Pay?\n",
            "A: To remove a card from Apple Pay, follow these steps based on your device:\n",
            "\n",
            "**On iPad or Apple Vision Pro:**\n",
            "1. Go to Settings > Wallet & Apple Pay.\n",
            "2. Tap the card you want to remove.\n",
            "3. Tap Remove Card.\n",
            "\n",
            "**On iPhone:**\n",
            "...\n",
            "Latency: 5440 ms\n",
            "---\n",
            "Q: Does Apple Pay work internationally and what are limits?\n",
            "A: Yes, Apple Pay works internationally in countries and regions that support contactless payments. However, the context does not specify any limits regarding its use abroad. For more detailed information, you may want to c...\n",
            "Latency: 1706 ms\n",
            "---\n",
            "Q: How do I contact Apple support for Apple Pay disputes?\n",
            "A: To contact Apple Support for Apple Pay disputes, you can call the Apple Support phone number for your country or region. For more information, visit the official Apple Support page [3]....\n",
            "Latency: 1557 ms\n",
            "---\n",
            "Latency P50: 2315.5 ms; P95: 6044 ms\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json, time, statistics\n",
        "\n",
        "def run_eval(top_k=6):\n",
        "    with open(\"backend/evaluation/queries.json\",\"r\") as f:\n",
        "        queries = json.load(f)\n",
        "    latencies = []\n",
        "    for q in queries:\n",
        "        t0 = time.time()\n",
        "        ctx = retrieve(q, top_k=top_k)\n",
        "        res = answer(q, ctx)\n",
        "        latencies.append(res[\"latency_ms\"])\n",
        "        # TODO: compute P@1, Recall@k, MRR against a labeled mapping of query→relevant URLs\n",
        "        print(f\"Q: {q}\\nA: {res['answer'][:220]}...\\nLatency: {res['latency_ms']} ms\\n---\")\n",
        "    print(\"Latency P50:\", statistics.median(latencies), \"ms; P95:\", sorted(latencies)[int(0.95*len(latencies))], \"ms\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31b951ec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: How do I add a new card to Apple Pay?\n",
            "A: To add a new card to Apple Pay, follow these steps based on your device:\n",
            "\n",
            "**On iPhone:**\n",
            "1. Open the Wallet app and tap the Add button.\n",
            "2. Tap \"Debit or Credit Card\" to add a new card.\n",
            "3. Tap Continue.\n",
            "4. Tap or hold you...\n",
            "F1=0.856, P@1=1.000, Recall@k=1.000, MRR=1.000, Latency=5546ms\n",
            "---\n",
            "Q: What fees apply when using Apple Pay?\n",
            "A: The provided context does not specify any fees associated with using Apple Pay. It mentions that Apple Pay is a service provided by Apple Payments Services LLC and that any card used in Apple Pay is offered by the card i...\n",
            "F1=0.898, P@1=0.000, Recall@k=0.000, MRR=0.000, Latency=1908ms\n",
            "---\n",
            "Q: How can I request a refund for an Apple Pay purchase?\n",
            "A: To request a refund for an Apple Pay purchase, you need to contact the merchant directly. Apple Pay does not handle refunds; the process is managed by the merchant from whom you made the purchase. If you have questions a...\n",
            "F1=0.765, P@1=0.000, Recall@k=0.000, MRR=0.000, Latency=1659ms\n",
            "---\n",
            "Q: Is Apple Pay supported on Apple Watch without iPhone?\n",
            "A: Yes, Apple Pay is supported on the Apple Watch without needing an iPhone. You can make purchases by double-clicking the side button on your Apple Watch, which opens your default card, and then holding the display near th...\n",
            "F1=0.873, P@1=0.000, Recall@k=0.000, MRR=0.000, Latency=1371ms\n",
            "---\n",
            "Q: What are Apple Pay security features like tokenization?\n",
            "A: Apple Pay employs several security features, including tokenization, to protect your payment information. When you add a card to Apple Pay, your card information is encrypted and sent to Apple servers. Apple then decrypt...\n",
            "F1=0.834, P@1=1.000, Recall@k=1.000, MRR=1.000, Latency=12224ms\n",
            "---\n",
            "Q: How do merchants integrate Apple Pay on the web?\n",
            "A: Merchants can integrate Apple Pay on the web by using an Apple Pay SDK or JavaScript API from a payment service provider (PSP). This is the quickest and most reliable method to support Apple Pay in apps and on websites. ...\n",
            "F1=0.846, P@1=1.000, Recall@k=1.000, MRR=1.000, Latency=3279ms\n",
            "---\n",
            "Q: Where can I find KYC or identity verification requirements?\n",
            "A: I don't have specific information on KYC or identity verification requirements for Apple Pay. I recommend contacting Apple Support for detailed assistance regarding this matter....\n",
            "F1=0.801, P@1=0.000, Recall@k=0.000, MRR=0.000, Latency=1616ms\n",
            "---\n",
            "Q: How do I remove a card from Apple Pay?\n",
            "A: To remove a card from Apple Pay, follow these steps based on your device:\n",
            "\n",
            "**On iPhone:**\n",
            "1. Open the Wallet app.\n",
            "2. Tap the card you want to remove.\n",
            "3. Tap the More button, then tap Card Details.\n",
            "4. Scroll down and tap ...\n",
            "F1=0.821, P@1=1.000, Recall@k=1.000, MRR=1.000, Latency=4207ms\n",
            "---\n",
            "Q: Does Apple Pay work internationally and what are limits?\n",
            "A: Yes, Apple Pay works internationally in various countries and regions where contactless payments are accepted. You can use it in stores, in apps, on the web, and for public transport in some areas. However, the availabil...\n",
            "F1=0.909, P@1=1.000, Recall@k=1.000, MRR=1.000, Latency=5274ms\n",
            "---\n",
            "Q: How do I contact Apple support for Apple Pay disputes?\n",
            "A: To contact Apple support for Apple Pay disputes, you can use the Apple Support app or call the Apple Support phone number for your country or region. You can find the specific phone number for your location on the offici...\n",
            "F1=0.869, P@1=0.000, Recall@k=0.000, MRR=0.000, Latency=2411ms\n",
            "---\n",
            "\n",
            "✅ Evaluation complete → evaluation_report.csv\n",
            "Average F1=0.8472, P@1=0.5000, Recall@k=0.5000, MRR=0.5000\n",
            "Latency P50=2845.0 ms; P95=12224 ms\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json\n",
        "import time\n",
        "import statistics\n",
        "import re\n",
        "import csv\n",
        "from typing import List, Dict\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "threshold = 0.65\n",
        "\n",
        "# ---------------- Tokenization ----------------\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    return re.findall(r\"\\w+\", text.lower())\n",
        "\n",
        "def answer_f1(predicted: str, reference: str) -> float:\n",
        "    \"\"\"\n",
        "    Compute F1 using cosine similarity for precision and recall.\n",
        "    Precision = sim(pred, ref) / sim(pred, pred)\n",
        "    Recall    = sim(pred, ref) / sim(ref, ref)\n",
        "    \"\"\"\n",
        "    if not predicted.strip() or not reference.strip():\n",
        "        return 0.0\n",
        "\n",
        "    # Embed both answers\n",
        "    embeddings = model.encode([predicted, reference])\n",
        "    pred_emb, ref_emb = embeddings[0], embeddings[1]\n",
        "\n",
        "    # Cosine similarities\n",
        "    sim_pred_ref = cosine_similarity([pred_emb], [ref_emb])[0][0]\n",
        "    sim_pred_pred = cosine_similarity([pred_emb], [pred_emb])[0][0]  # should be 1.0\n",
        "    sim_ref_ref = cosine_similarity([ref_emb], [ref_emb])[0][0]      # should be 1.0\n",
        "\n",
        "    # Compute precision and recall\n",
        "    precision = sim_pred_ref / sim_pred_pred if sim_pred_pred > 0 else 0\n",
        "    recall = sim_pred_ref / sim_ref_ref if sim_ref_ref > 0 else 0\n",
        "\n",
        "    # F1 formula\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "\n",
        "def precision_at_1(results: List[str], relevant: str) -> float:\n",
        "    if not results:\n",
        "        return 0.0\n",
        "    ref_emb = model.encode([relevant])\n",
        "    top_emb = model.encode([results[0]])\n",
        "    \n",
        "    sims=float(cosine_similarity(ref_emb, top_emb)[0][0])  # similarity score\n",
        "    if sims > threshold:  # threshold for \"relevant enough\"\n",
        "        return 1.0\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def recall_at_k(results: List[str], relevant: str) -> float:\n",
        "    if not results:\n",
        "        return 0.0\n",
        "    ref_emb = model.encode([relevant])\n",
        "    retrieved_embs = model.encode(results)\n",
        "    sims = cosine_similarity(ref_emb, retrieved_embs)[0]\n",
        "    sims=float(max(sims))  # best similarity among top-k\n",
        "    if sims > threshold:  # threshold for \"relevant enough\"\n",
        "        return 1.0\n",
        "    return 0.0    \n",
        "\n",
        "def mrr(results: List[str], relevant: str) -> float:\n",
        "    if not results:\n",
        "        return 0.0\n",
        "    ref_emb = model.encode([relevant])\n",
        "    retrieved_embs = model.encode(results)\n",
        "    sims = cosine_similarity(ref_emb, retrieved_embs)[0]\n",
        "    # Rank by similarity\n",
        "    #sorted_indices = np.argsort(-sims)\n",
        "    for idx in range(1,len(sims)+1):\n",
        "        if sims[idx-1] > threshold:  # threshold for \"relevant enough\"\n",
        "            return 1.0 / idx\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "# ---------------- Ground Truth ----------------\n",
        "GROUND_TRUTH = {\n",
        "    \"How do I add a new card to Apple Pay?\": \"Open the Wallet app on your iPhone, tap the Add (+) button, and follow the prompts to add a credit or debit card. You can also add cards through the Apple Watch app for your watch.\",\n",
        "    \"What fees apply when using Apple Pay?\": \"Apple does not charge fees for using Apple Pay. However, your bank or card issuer may apply standard transaction or international fees.\",\n",
        "    \"How can I request a refund for an Apple Pay purchase?\": \"Refunds are handled by the merchant. Contact the store or app where you made the purchase; the refund will return to your original payment card in Wallet.\",\n",
        "    \"Is Apple Pay supported on Apple Watch without iPhone?\": \"Yes, once Apple Pay is set up on your Apple Watch, you can make payments without your iPhone nearby. Initial setup requires pairing with an iPhone.\",\n",
        "    \"What are Apple Pay security features like tokenization?\": \"Apple Pay uses tokenization, device-specific numbers, and dynamic security codes. Your actual card number is never shared with merchants or stored on Apple servers.\",\n",
        "    \"How do merchants integrate Apple Pay on the web?\": \"Merchants can enable Apple Pay by using Apple Pay JS API and configuring their payment processor to support Apple Pay. They must register with Apple and verify their domain.\",\n",
        "    \"Where can I find KYC or identity verification requirements?\": \"KYC requirements apply to Apple Cash and some bank cards. Verification is done through your bank or Apple Cash setup in Wallet, where you may need to provide personal details.\",\n",
        "    \"How do I remove a card from Apple Pay?\": \"Open the Wallet app, select the card, tap More (…), and choose Remove Card. On Apple Watch, use the Watch app on iPhone or remove directly from the watch.\",\n",
        "    \"Does Apple Pay work internationally and what are limits?\": \"Apple Pay works in most countries where contactless payments are accepted. Limits depend on local regulations and your card issuer, not Apple Pay itself.\",\n",
        "    \"How do I contact Apple support for Apple Pay disputes?\": \"Visit support.apple.com/apple-pay or use the Apple Support app. For transaction disputes, contact your bank or card issuer first.\"\n",
        "}\n",
        "\n",
        "# ---------------- Evaluation Loop ----------------\n",
        "# from rag_pipeline import retrieve, answer  # Uncomment in your environment\n",
        "\n",
        "def run_eval(top_k=6, output_csv=\"evaluation_report.csv\"):\n",
        "    with open(\"backend/evaluation/queries.json\", \"r\") as f:\n",
        "        queries = json.load(f)\n",
        "\n",
        "    latencies = []\n",
        "    rows = []\n",
        "    f1_scores, p1_scores, recall_scores, mrr_scores = [], [], [], []\n",
        "\n",
        "    for q in queries:\n",
        "        t0 = time.time()\n",
        "        ctx = retrieve(q, top_k=top_k)  # Retrieves top_k chunks\n",
        "        res = answer(q, ctx)            # Generates answer using RAG\n",
        "        latency = res[\"latency_ms\"]\n",
        "        latencies.append(latency)\n",
        "\n",
        "        predicted = res[\"answer\"]\n",
        "        reference = GROUND_TRUTH.get(q, \"\")\n",
        "        retrieved_texts = [c[\"text\"] for c in ctx]\n",
        "\n",
        "        # Compute metrics\n",
        "        f1 = answer_f1(predicted, reference)\n",
        "        p1 = precision_at_1(retrieved_texts, reference)\n",
        "        rec = recall_at_k(retrieved_texts, reference)\n",
        "        rr = mrr(retrieved_texts, reference)\n",
        "\n",
        "        f1_scores.append(f1)\n",
        "        p1_scores.append(p1)\n",
        "        recall_scores.append(rec)\n",
        "        mrr_scores.append(rr)\n",
        "\n",
        "        rows.append([q, f1, p1, rec, rr, latency])\n",
        "\n",
        "        print(f\"Q: {q}\\nA: {predicted[:220]}...\\nF1={f1:.3f}, P@1={p1:.3f}, Recall@k={rec:.3f}, MRR={rr:.3f}, Latency={latency}ms\\n---\")\n",
        "\n",
        "    # Write CSV\n",
        "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Query\", \"Answer F1\", \"Precision@1\", \"Recall@k\", \"MRR\", \"Latency(ms)\"])\n",
        "        writer.writerows(rows)\n",
        "        writer.writerow([])\n",
        "        writer.writerow([\"Average\",\n",
        "                         statistics.mean(f1_scores),\n",
        "                         statistics.mean(p1_scores),\n",
        "                         statistics.mean(recall_scores),\n",
        "                         statistics.mean(mrr_scores),\n",
        "                         statistics.median(latencies)])\n",
        "    print(f\"\\n✅ Evaluation complete → {output_csv}\")\n",
        "    print(f\"Average F1={statistics.mean(f1_scores):.4f}, P@1={statistics.mean(p1_scores):.4f}, Recall@k={statistics.mean(recall_scores):.4f}, MRR={statistics.mean(mrr_scores):.4f}\")\n",
        "    print(f\"Latency P50={statistics.median(latencies)} ms; P95={sorted(latencies)[int(0.95*len(latencies))]} ms\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_eval()\n",
        "\n",
        "# minilm_embed: Average F1=0.7774, P@1=0.3000, Recall@k=0.6000, MRR=0.4033,Latency P50=1567.0 ms; P95=2454 ms, cost: 0.1$\n",
        "#e5 embed: Average F1=0.8150, P@1=0.6000, Recall@k=0.8000, MRR=0.6833, Latency P50=1899.5 ms; P95=3402 ms, cost 0.15$\n",
        "#bge embed: Average F1=0.7674, P@1=0.5000, Recall@k=0.6000, MRR=0.5333, Latency P50=1144.0 ms; P95=1928 ms, cost 0.15$\n",
        "\n",
        "#e5\n",
        "#semantic: Average F1=0.8150, P@1=0.6000, Recall@k=0.8000, MRR=0.6833, Latency P50=1899.5 ms; P95=3402 ms,(2.5 sec) cost 0.1$\n",
        "#fixed 256,0: Average F1=0.8274, P@1=0.5000, Recall@k=0.7000, MRR=0.5833, Latency P50=1995.0 ms; P95=4886 ms, 2.9sec\n",
        "#fixee 256,64: Average F1=0.8049, P@1=0.5000, Recall@k=0.6000, MRR=0.5333,Latency P50=3080.5 ms; P95=5507 ms 3.6 sec\n",
        "#fixed 256,128: Average F1=0.8231, P@1=0.5000, Recall@k=0.7000, MRR=0.5833, Latency P50=3752.5 ms; P95=6097 ms 4 sec\n",
        "#fixed 512,0: Average F1=0.8338, P@1=0.5000, Recall@k=0.6000, MRR=0.5500, Latency P50=3005.5 ms; P95=5658 ms  3.5sec\n",
        "#fixed 512,64: Average F1=0.8278, P@1=0.4000, Recall@k=0.5000, MRR=0.4500, Latency P50=2938.5 ms; P95=4475 ms 3.5 sec\n",
        "#fixed 512,128: Average F1=0.7967, P@1=0.4000, Recall@k=0.6000, MRR=0.4833, Latency P50=2801.5 ms; P95=6072 ms 3.5sec\n",
        "#llm chunks: Average F1=0.8351, P@1=0.6000, Recall@k=0.7000, MRR=0.6250,Latency P50=1900.0 ms; P95=5205 ms 2.6 sec\n",
        "#recursive: Average F1=0.8039, P@1=0.6000, Recall@k=0.8000, MRR=0.6450, Latency P50=1390.5 ms; P95=2789 ms 2.2 sec\n",
        "#structure: Average F1=0.8472, P@1=0.5000, Recall@k=0.5000, MRR=0.5000,Latency P50=2845.0 ms; P95=12224 ms 4.3 sec"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "LLM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
